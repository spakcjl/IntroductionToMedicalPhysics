---
title: "Medical Physics 1: Signals and Systems"
author: ""
date: "`r Sys.Date()`"
output: powerpoint_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(ggplot2)
library(dplyr)
library(reticulate)
use_python(".venv/bin/python")
```

# Introduction to Signals and Systems

## What are Signals?

- Signals are functions of one or more independent variables that contain information about a physical phenomenon.
- We represent signals mathematically as functions, like $x(t)$ for continuous-time signals or $x[n]$ for discrete-time signals.

***

- **Continuous-Time (CT) Signals:** The independent variable is continuous. Examples include audio recordings and voltage in a circuit.
- **Discrete-Time (DT) Signals:** The independent variable takes on only a discrete set of values. Examples include daily stock prices or digital images.



::: notes
Introduce the fundamental concepts of signals and systems. Use the examples from chapter1.md (RC circuit, automobile, speech) to illustrate the diversity of signals in the real world. Emphasize that the mathematical framework we'll be learning is applicable across many different fields.
:::

## Visualizing Signals

```{r speech_vs_wind, fig.height=4, fig.width=8}
# Simulate a speech-like signal
time <- seq(0, 0.5, length.out = 1000)
speech_signal <- sin(2 * pi * 50 * time) * exp(-10 * time) + rnorm(1000, 0, 0.1)
speech_data <- data.frame(Time = time, Amplitude = speech_signal, Type = "Speech Signal")

# Simulate a vertical wind profile
height <- seq(0, 1000, length.out = 1000)
wind_speed <- 5 * log(height + 1) + rnorm(1000, 0, 0.5)
wind_data <- data.frame(Time = height / 1000, Amplitude = wind_speed, Type = "Vertical Wind Profile") # Normalize height to time for plotting

# Combine and plot
combined_data <- rbind(speech_data, wind_data)
ggplot(combined_data, aes(x = Time, y = Amplitude)) +
  geom_line() +
  facet_wrap(~Type, scales = "free") +
  labs(title = "Signal Examples: Speech vs. Wind Profile",
       x = "Independent Variable (Time / Height)",
       y = "Amplitude") +
  theme_minimal()
```



::: notes
This slide illustrates two different types of signals. The speech signal on the left is characterized by rapid, high-frequency variations. This is typical of audio signals. On the right, the vertical wind profile shows a much smoother, slowly varying curve. This highlights the vast range of behaviors that signals can exhibit.
:::

## Continuous vs. Discrete Signals

```{r ct_vs_dt, fig.height=4, fig.width=8}
# Continuous-time signal
t_cont <- seq(0, 2, length.out = 400)
x_cont <- sin(2 * pi * 1.5 * t_cont)
ct_data <- data.frame(Time = t_cont, Amplitude = x_cont)

# Discrete-time signal (sampled)
n_disc <- 0:20
t_disc <- n_disc * 0.1
x_disc <- sin(2 * pi * 1.5 * t_disc)
dt_data <- data.frame(Time = t_disc, Amplitude = x_disc, n = n_disc)

# Plot
ggplot() +
  geom_line(data = ct_data, aes(x = Time, y = Amplitude, color = "Continuous x(t)"), linetype = "dashed") +
  geom_point(data = dt_data, aes(x = Time, y = Amplitude, color = "Discrete x[n]"), size = 3) +
  geom_segment(data = dt_data, aes(x = Time, xend = Time, y = 0, yend = Amplitude, color = "Discrete x[n]")) +
  scale_color_manual(values = c("Continuous x(t)" = "lightblue", "Discrete x[n]" = "red")) +
  labs(title = "Continuous-Time (CT) vs. Discrete-Time (DT) Signals",
       x = "Time (t) or Sample (n)",
       y = "Amplitude",
       color = "Signal Type") +
  theme_minimal()
```



::: notes
Here we see the fundamental difference between CT and DT signals. The blue dashed line represents a continuous sine wave, defined for all values of time 't'. The red points represent a discrete-time signal, obtained by "sampling" the continuous signal at regular intervals. Notice the discrete signal is only defined at these specific integer points 'n'. A major theme in this course will be understanding the relationship between a CT signal and the DT signal derived from it.
:::

## Signal Energy

- We often use concepts of "energy" and "power" to characterize signals, drawing an analogy from electrical circuits.
- **Total Energy** over an infinite interval for a CT signal $x(t)$ is:
  $$ E_{\infty} = \int_{-\infty}^{\infty} |x(t)|^2 dt $$

## Average Signal Power

- **Average Power** over an infinite interval is:
  $$ P_{\infty} = \lim_{T \to \infty} \frac{1}{2T} \int_{-T}^{T} |x(t)|^2 dt $$
- Similar definitions exist for discrete-time signals. A signal is an **energy signal** if $E_{\infty} < \infty$. A signal is a **power signal** if $0 < P_{\infty} < \infty$.



::: notes
It's important to clarify that these are generalized definitions. The terms 'energy' and 'power' might not correspond to physical energy and power in all contexts. For instance, if x(t) is voltage, the formula for physical energy would require dividing by resistance. However, these definitions provide a consistent way to measure the "size" or "strength" of a signal.
:::

## Instantaneous Power Example

```{python power_plot, fig.height=4, fig.width=8}
import numpy as np
import matplotlib.pyplot as plt

# Define the voltage signal: a decaying exponential
R = 1.0  # Resistance in Ohms
v0 = 5.0 # Initial voltage
a = 2.0  # Decay rate
t = np.linspace(0, 3, 500)
v_t = v0 * np.exp(-a * t)

# Calculate instantaneous power: p(t) = v^2(t) / R
p_t = (v_t**2) / R

# Plotting
plt.figure(figsize=(10, 5))
plt.plot(t, p_t, label='$p(t) = v^2(t)/R$')
plt.title('Instantaneous Power of a Decaying Exponential Voltage')
plt.xlabel('Time (t)')
plt.ylabel('Power (Watts)')
plt.grid(True)
plt.legend()
plt.show()
```



::: notes
This plot shows the instantaneous power dissipated by a resistor when a decaying exponential voltage is applied. As the voltage across the resistor decreases exponentially, the power dissipated decreases even faster, following the square of the voltage. This is an example of an energy signal, where the total energy is finite.
:::

## Total Energy Calculation

- The **Total Energy** of a signal is the integral (or sum for DT) of its squared magnitude.
- For a finite-duration signal, this represents the total energy it contains.
- Let's visualize how this energy accumulates over time.

## Energy Accumulation Plot

```{r energy_accumulation, fig.height=4, fig.width=8}
# Define a finite signal (e.g., a rectangular pulse)
time <- seq(-1, 3, length.out = 1000)
signal <- ifelse(time >= 0 & time <= 2, 2, 0)
signal_data <- data.frame(Time = time, Signal = signal)

# Calculate cumulative energy
cumulative_energy <- cumsum(signal^2) * (time[2] - time[1])
energy_data <- data.frame(Time = time, CumulativeEnergy = cumulative_energy)

# Plot Signal and its Cumulative Energy
p1 <- ggplot(signal_data, aes(x = Time, y = Signal)) +
  geom_line() +
  labs(title = "A Finite Signal x(t)", y = "Amplitude") +
  theme_minimal()

p2 <- ggplot(energy_data, aes(x = Time, y = CumulativeEnergy)) +
  geom_line(color = "blue") +
  labs(title = "Accumulated Energy E(t)", y = "Energy") +
  theme_minimal()

# Using patchwork to combine plots
if(!require(patchwork)) install.packages("patchwork")
library(patchwork)
p1 + p2
```



::: notes
On the left, we have a simple rectangular pulse signal that is non-zero only between t=0 and t=2. On the right, we see the accumulated energy of this signal. The energy starts at zero, increases quadratically while the pulse is active, and then holds at its maximum value after the pulse ends. This maximum value represents the total energy of the signal.
:::

## Transformations of the Independent Variable

- We can transform a signal by changing its independent variable. The most common transformation is a **time shift**.
- A signal $x(t-t_0)$ is a time-shifted version of $x(t)$.
  - If $t_0 > 0$, the signal is delayed (shifted right).
  - If $t_0 < 0$, the signal is advanced (shifted left).
- Similarly for discrete-time signals, $x[n-n_0]$.



::: notes
Time shifts are fundamental in signal processing. They arise in applications like radar and sonar, where a transmitted signal is received at a later time. Understanding how to mathematically represent and manipulate these shifts is a crucial first step in system analysis.
:::

## Visualizing Time Shifts

```{python time_shift_plot, fig.height=4, fig.width=10}
import numpy as np
import matplotlib.pyplot as plt

def triangular_pulse(t, width=2):
    return np.maximum(0, 1 - np.abs(t / (width/2)))

t = np.linspace(-5, 5, 500)
x_t = triangular_pulse(t)
x_t_delayed = triangular_pulse(t - 2)  # Shift right by 2
x_t_advanced = triangular_pulse(t + 1.5) # Shift left by 1.5

fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4), sharey=True)

ax1.plot(t, x_t)
ax1.set_title('Original Signal, $x(t)$')
ax1.grid(True)

ax2.plot(t, x_t_delayed)
ax2.set_title('Delayed Signal, $x(t-2)$')
ax2.grid(True)

ax3.plot(t, x_t_advanced)
ax3.set_title('Advanced Signal, $x(t+1.5)$')
ax3.grid(True)

plt.suptitle('Time Shifting a Signal')
plt.show()
```



::: notes
This slide visually demonstrates the concept of time shifting. The first plot shows our original signal, a simple triangular pulse centered at zero. The middle plot shows the same signal delayed, or shifted to the right, by 2 units. Every point on the original signal now occurs 2 units later. The third plot shows the signal advanced, or shifted to the left, by 1.5 units.
:::

## Combined Transformations

- We can combine shifting, reversal, and scaling.
- The operation $x(at+b)$ can be visualized by first shifting $x(t)$ by $b$ to get $x(t+b)$, and then scaling the result by $a$.
- Order matters! Shifting then scaling is different from scaling then shifting.

```{python combined_transform_plot, fig.height=4, fig.width=10}
import numpy as np
import matplotlib.pyplot as plt

def triangular_pulse(t, width=2):
    return np.maximum(0, 1 - np.abs(t / (width/2)))

t = np.linspace(-5, 5, 500)

# Original signal
x_t = triangular_pulse(t-1) # Shifted to start

# x(2t+1)
x_2t_plus_1 = triangular_pulse(2*t+1)

# x(2(t+1))
x_2_t_plus_1 = triangular_pulse(2*(t+0.5))


fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4), sharey=True)

ax1.plot(t, x_t)
ax1.set_title('Original Signal, $x(t-1)$')
ax1.grid(True)

ax2.plot(t, x_2t_plus_1)
ax2.set_title('Shift then Scale: $x(2t+1)$')
ax2.grid(True)

ax3.plot(t, x_2_t_plus_1)
ax3.set_title('Scale then Shift: $x(2(t+0.5))$')
ax3.grid(True)


plt.suptitle('Combined Transformations')
plt.show()
```



::: notes
This slide shows the importance of the order of operations. The original signal is a triangular pulse shifted to be centered at t=1. The middle plot shows the result of replacing t with 2t+1. The final plot shows scaling first, then shifting. The results are different.
:::

# Transformations of the Independent Variable

## Time Reversal

- Another common transformation is **time reversal**.
- The signal $x(-t)$ is the version of $x(t)$ reflected about the origin ($t=0$).
- If you were to play a tape recording of $x(t)$ backward, you would hear $x(-t)$.



::: notes
Time reversal is like looking in a mirror placed at time zero. The future becomes the past, and the past becomes the future. This operation is straightforward mathematically but has profound implications for concepts like causality, which we will discuss later.
:::

## Visualizing Time Reversal

```{python time_reversal_plot, fig.height=4, fig.width=8}
import numpy as np
import matplotlib.pyplot as plt

def asymmetric_triangle(t):
    # Ramps up from t=-1 to t=1, then ramps down from t=1 to t=2
    return np.piecewise(t, [t < -1, (t >= -1) & (t < 1), (t >= 1) & (t < 2), t >= 2],
                        [0, lambda t: (t + 1) / 2, lambda t: 1 - (t - 1), 0])

t = np.linspace(-3, 3, 600)
x_t = asymmetric_triangle(t)
x_neg_t = asymmetric_triangle(-t) # Time-reversed signal

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4), sharey=True)

ax1.plot(t, x_t)
ax1.set_title('Original Signal, $x(t)$')
ax1.grid(True)
ax1.set_xlabel('Time (t)')
ax1.set_ylabel('Amplitude')


ax2.plot(t, x_neg_t)
ax2.set_title('Time-Reversed Signal, $x(-t)$')
ax2.grid(True)
ax2.set_xlabel('Time (t)')


plt.suptitle('Time Reversal')
plt.show()
```



::: notes
Here, the signal on the left is an asymmetric triangle. The signal on the right is its time-reversed counterpart. Notice how the ramp up from -1 to 1 in the original signal becomes a ramp down from -1 to 1 in the reversed signal, and the ramp down from 1 to 2 becomes a ramp up from -2 to -1.
:::

## Time Scaling

- **Time Scaling** compresses or stretches a signal in time.
- Consider $x(at)$:
  - If $|a| > 1$, the signal is **compressed** (it happens faster).
  - If $0 < |a| < 1$, the signal is **stretched** (it happens slower).
- Think of playing a tape at a different speed. $x(2t)$ is twice the speed; $x(0.5t)$ is half the speed.



::: notes
Time scaling is a very intuitive concept. Compressing a signal means the events within the signal occur more quickly, while stretching it means they occur more slowly. Pay close attention to the notation: the factor 'a' appears inside the function argument, and its effect is somewhat counter-intuitive. A larger 'a' leads to compression, not stretching.
:::

## Visualizing Time Scaling

```{python time_scaling_plot, fig.height=4, fig.width=10}
import numpy as np
import matplotlib.pyplot as plt

def triangular_pulse(t, width=2):
    return np.maximum(0, 1 - np.abs(t / (width/2)))

t = np.linspace(-5, 5, 500)
x_t = triangular_pulse(t)
x_2t = triangular_pulse(2 * t)      # Compressed
x_0_5t = triangular_pulse(0.5 * t)  # Stretched

fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4), sharey=True)

ax1.plot(t, x_t)
ax1.set_title('Original Signal, $x(t)$')
ax1.grid(True)
ax1.set_xlabel('Time (t)')
ax1.set_ylabel('Amplitude')

ax2.plot(t, x_2t)
ax2.set_title('Compressed Signal, $x(2t)$')
ax2.grid(True)
ax2.set_xlabel('Time (t)')

ax3.plot(t, x_0_5t)
ax3.set_title('Stretched Signal, $x(0.5t)$')
ax3.grid(True)
ax3.set_xlabel('Time (t)')

plt.suptitle('Time Scaling a Signal')
plt.show()
```



::: notes
The original triangular pulse has a width of 2 (from -1 to 1). The compressed signal, x(2t), has its width halved to 1 (from -0.5 to 0.5). The stretched signal, x(0.5t), has its width doubled to 4 (from -2 to 2). This demonstrates the compression and stretching effect of time scaling.
:::

## Periodic vs. Aperiodic Signals

- A continuous-time signal $x(t)$ is **periodic** if there is a value $T > 0$ such that:
  $$ x(t) = x(t+T) \quad \text{for all } t $$
- The smallest such $T$ is the **fundamental period**.
- A discrete-time signal $x[n]$ is periodic if there is an integer $N > 0$ such that:
  $$ x[n] = x[n+N] \quad \text{for all } n $$
- If a signal is not periodic, it is **aperiodic**.



::: notes
Periodicity is a crucial concept, especially when we get to Fourier analysis. A periodic signal repeats itself exactly over a certain duration, the period. An aperiodic signal never repeats. Many signals in nature can be modeled as sums of periodic signals.
:::

## Visualizing Periodicity

```{r periodic_plot, fig.height=4, fig.width=8}
t <- seq(0, 20, length.out = 2000)
periodic_signal <- cos(t)
aperiodic_signal <- cos(t) + sin(sqrt(2) * t)

periodic_data <- data.frame(Time = t, Amplitude = periodic_signal, Type = "Periodic: cos(t)")
aperiodic_data <- data.frame(Time = t, Amplitude = aperiodic_signal, Type = "Aperiodic: cos(t) + sin(sqrt(2)t)")

combined_data <- rbind(periodic_data, aperiodic_data)

ggplot(combined_data, aes(x = Time, y = Amplitude)) +
  geom_line() +
  facet_wrap(~Type, scales = "free_y", nrow = 2) +
  labs(title = "Periodic vs. Aperiodic Signals",
       x = "Time (t)",
       y = "Amplitude") +
  theme_minimal()
```



::: notes
The top plot shows cos(t), which is clearly periodic. It repeats every 2*pi units. The bottom plot shows the sum of two sinusoids whose frequencies do not have a rational ratio (1 and sqrt(2)). The result is a signal that never exactly repeats its values, making it aperiodic.
:::

## Even and Odd Signals

- A signal is **even** if it is symmetric about the vertical axis.
  $$ x(t) = x(-t) \quad \text{or} \quad x[n] = x[-n] $$
- A signal is **odd** if it is anti-symmetric about the origin.
  $$ x(t) = -x(-t) \quad \text{or} \quad x[n] = -x[-n] $$
- Any signal can be decomposed into a sum of an even part and an odd part.
  $$ x(t) = Ev\{x(t)\} + Od\{x(t)\} $$
  $$ Ev\{x(t)\} = \frac{1}{2}[x(t) + x(-t)] $$
  $$ Od\{x(t)\} = \frac{1}{2}[x(t) - x(-t)] $$



::: notes
The concept of even and odd signals is a form of symmetry that simplifies analysis. The decomposition is unique and powerful. Knowing how a system responds to even or odd signals can tell us a lot about its overall behavior.
:::

## Even/Odd Decomposition Example

```{r even_odd_plot, fig.height=6, fig.width=8}
# Create an arbitrary discrete signal
n <- -5:5
x_n <- c(0, 0, 1, 2.5, 3, 1, 0, 0, 0.5, 1.5, 0)
signal_df <- data.frame(n = n, x_n = x_n)

# Decompose
x_neg_n <- rev(x_n) # Approximate x[-n]
even_part <- 0.5 * (x_n + x_neg_n)
odd_part <- 0.5 * (x_n - x_neg_n)

# Create data frames for plotting
plot_df <- data.frame(
  n = rep(n, 3),
  value = c(x_n, even_part, odd_part),
  type = factor(rep(c("x[n] (Original)", "Ev{x[n]} (Even Part)", "Od{x[n]} (Odd Part)"), each = 11),
                levels = c("x[n] (Original)", "Ev{x[n]} (Even Part)", "Od{x[n]} (Odd Part)"))
)

# Plot
ggplot(plot_df, aes(x = n, y = value)) +
  geom_segment(aes(xend = n, yend = 0)) +
  geom_point(size = 3) +
  facet_wrap(~type, ncol = 1) +
  labs(title = "Even/Odd Decomposition of a Discrete Sequence",
       x = "Sample (n)",
       y = "Amplitude") +
  theme_minimal()
```



::: notes
The top plot shows an arbitrary sequence. The middle plot shows its even part, which you can see is perfectly symmetric around n=0. The bottom plot shows its odd part, which is anti-symmetric. If you add the even and odd parts together point by point, you will reconstruct the original signal.
:::

# Exponential and Sinusoidal Signals

## The Complex Exponential Signal

- One of the most important signals in this course is the continuous-time complex exponential:
  $$ x(t) = C e^{at} $$
- The parameters $C$ and $a$ can be complex numbers.
- The behavior of this signal changes dramatically depending on the values of $C$ and $a$.



::: notes
This signal is a fundamental building block. We will see that a huge class of signals can be represented as a sum of complex exponentials. Understanding their properties is key to the rest of the course. We will first look at the case where 'a' is purely real.
:::

## Real Exponentials ($a$ is real)

```{python real_exp_plot, fig.height=5, fig.width=8}
import numpy as np
import matplotlib.pyplot as plt

t = np.linspace(0, 2, 400)
C = 1.5

plt.figure(figsize=(10, 6))

# Growing exponentials (a > 0)
for a in [0.5, 1.0, 2.0]:
    plt.plot(t, C * np.exp(a * t), linestyle='-', label=f'$a = {a}$')

# Decaying exponentials (a < 0)
for a in [-0.5, -1.0, -2.0]:
    plt.plot(t, C * np.exp(a * t), linestyle='--', label=f'$a = {a}$')

# Constant (a = 0)
plt.plot(t, C * np.exp(0 * t), linestyle=':', label='$a = 0$')


plt.title('Family of Real Exponential Curves $x(t) = Ce^{at}$')
plt.xlabel('Time (t)')
plt.ylabel('Amplitude')
plt.grid(True)
plt.legend(title="Value of 'a'")
plt.ylim(0, C * np.exp(2*2))
plt.show()
```



::: notes
This plot shows the behavior of the real exponential signal for a fixed C. When 'a' is positive (solid lines), the signal grows exponentially. The larger the value of 'a', the faster the growth. When 'a' is negative (dashed lines), the signal decays exponentially. The more negative 'a' is, the faster the decay. When a=0, the signal is a constant.
:::

## Periodic Complex Exponentials ($a = j\omega$)

- When $a$ is purely imaginary, we get a periodic complex exponential:
  $$ x(t) = e^{j\omega_0 t} $$
- By Euler's relation, we can decompose this into real and imaginary sinusoidal parts:
  $$ e^{j\omega_0 t} = \cos(\omega_0 t) + j\sin(\omega_0 t) $$
- This signal is periodic with fundamental period $T_0 = \frac{2\pi}{|\omega_0|}$.



::: notes
This is one of the most important signals in the entire field. The real and imaginary parts are sinusoids, which are fundamental to describing oscillations and periodic phenomena. The frequency is determined by omega_0.
:::

## The Sinusoidal Signal

- The sinusoidal signal is closely related:
  $$ x(t) = A\cos(\omega_0 t + \phi) $$
- It can be expressed in terms of complex exponentials:
  $$ A\cos(\omega_0 t + \phi) = \frac{A}{2}e^{j(\omega_0 t + \phi)} + \frac{A}{2}e^{-j(\omega_0 t + \phi)} $$
- It is a real-valued signal that is periodic with period $T_0 = \frac{2\pi}{|\omega_0|}$.



::: notes
We will frequently move back and forth between the sinusoidal representation and the complex exponential representation. The complex exponential form is often easier to manipulate mathematically, but the sinusoid is what we often measure in the real world (e.g., voltage, pressure).
:::

## Visualizing $e^{j\omega t}$

```{r complex_exp_plot, fig.height=5, fig.width=8}
library(tidyr)
t <- seq(0, 4*pi, length.out = 1000)
omega0 <- 1
x_t <- exp(1i * omega0 * t)

plot_data <- data.frame(
  Time = t,
  Real_Part = Re(x_t),
  Imaginary_Part = Im(x_t)
) %>%
  pivot_longer(cols = c("Real_Part", "Imaginary_Part"), names_to = "Part", values_to = "Amplitude")

ggplot(plot_data, aes(x = Time, y = Amplitude, color = Part)) +
  geom_line() +
  facet_wrap(~Part, nrow=2) +
  labs(title = "Real and Imaginary Parts of $e^{j\\omega t}$",
       x = "Time (t)",
       y = "Amplitude") +
  scale_color_manual(values = c("Real_Part" = "blue", "Imaginary_Part" = "red")) +
  theme_minimal() +
  theme(legend.position = "none")
```



::: notes
This plot shows the real part (cosine) and the imaginary part (sine) of the complex exponential signal. They are both sinusoids, but they are 90 degrees out of phase with each other. When the real part is at a maximum, the imaginary part is zero, and vice-versa.
:::

## Harmonically Related Exponentials

- A set of complex exponentials is **harmonically related** if their fundamental frequencies are all integer multiples of a single positive frequency, $\omega_0$.
  $$ \phi_k(t) = e^{jk\omega_0 t}, \quad k = 0, \pm 1, \pm 2, \dots $$
- The signal for $k=1$ is the **fundamental**, and the signal for $k=N$ is the **Nth harmonic**.
- All signals in a harmonically related set are periodic with period $T_0 = 2\pi/\omega_0$.



::: notes
The concept of harmonics is fundamental to music and, as we'll see in Chapter 3, to the analysis of any periodic signal. It means that complex periodic signals can be broken down into a sum of simpler signals whose frequencies are neatly related as integer multiples.
:::

## Visualizing Harmonics

```{r harmonics_plot, fig.height=6, fig.width=8}
t <- seq(0, 2*pi, length.out = 1000)
omega0 <- 2
k_values <- c(1, 2, 3)

# Create data for each harmonic
harmonic_data <- lapply(k_values, function(k) {
  data.frame(
    Time = t,
    Amplitude = cos(k * omega0 * t),
    Harmonic = paste0(k, "st Harmonic (k=", k, ")")
  )
}) %>% bind_rows()

harmonic_data$Harmonic <- factor(harmonic_data$Harmonic, levels = unique(harmonic_data$Harmonic))

# Plot
ggplot(harmonic_data, aes(x = Time, y = Amplitude)) +
  geom_line() +
  facet_wrap(~Harmonic, ncol = 1) +
  labs(title = "Harmonically Related Sinusoidal Signals (ω₀=2)",
       x = "Time (t)",
       y = "Amplitude") +
  theme_minimal()
```



::: notes
This plot shows the first three harmonics of a signal with a fundamental frequency of 2. The top plot (k=1) shows the fundamental signal, which completes two cycles in this interval. The middle plot (k=2) is the second harmonic, which oscillates twice as fast. The bottom plot (k=3) is the third harmonic, oscillating three times as fast. All are periodic with the same fundamental period.
:::

## General Complex Exponentials ($a = r + j\omega$)

- When $a$ is a general complex number, we combine the real and periodic cases.
  $$ x(t) = C e^{(r + j\omega_0)t} = C e^{rt} e^{j\omega_0 t} $$
- This can be broken down into real and imaginary parts:
  $$ x(t) = C e^{rt} (\cos(\omega_0 t) + j\sin(\omega_0 t)) $$
- This represents a sinusoid whose amplitude is growing or decaying exponentially.



::: notes
This is the most general form of the complex exponential. It's a powerful signal that can represent pure growth/decay, pure oscillation, or, most interestingly, oscillations that grow or decay in amplitude over time.
:::

## Damped Sinusoids

- A common case is the **damped sinusoid**, where the real part of $a$ is negative ($r < 0$).
- These signals are sinusoids multiplied by a decaying exponential.
- They are prevalent in physical systems with energy dissipation, like RLC circuits or mechanical systems with friction.



::: notes
Damped sinusoids are what you see in the real world when something oscillates and then settles down. Think of a guitar string being plucked or a car's suspension after hitting a bump. The oscillation is the sinusoidal part, and the settling down is the exponential decay.
:::

## Visualizing a Damped Sinusoid

```{python damped_sinusoid_plot, fig.height=5, fig.width=8}
import numpy as np
import matplotlib.pyplot as plt

# Parameters
A = 5.0    # Initial amplitude
a = 0.5    # Decay rate (r)
omega = 5  # Angular frequency

t = np.linspace(0, 10, 500)
x_t = A * np.exp(-a * t) * np.cos(omega * t)
envelope_pos = A * np.exp(-a * t)
envelope_neg = -A * np.exp(-a * t)

plt.figure(figsize=(10, 6))
plt.plot(t, x_t, label='$x(t) = Ae^{-at}\\cos(\\omega t)$')
plt.plot(t, envelope_pos, 'r--', label='Envelope: $Ae^{-at}$')
plt.plot(t, envelope_neg, 'r--')
plt.title('Damped Sinusoid and its Exponential Envelope')
plt.xlabel('Time (t)')
plt.ylabel('Amplitude')
plt.grid(True)
plt.legend()
plt.show()
```



::: notes
This plot clearly illustrates the concept of a damped sinusoid. The blue curve is the signal itself, oscillating back and forth. The red dashed lines form the exponential envelope. The amplitude of the oscillations is bounded by this decaying envelope, showing how the signal loses energy over time.
:::

## Discrete-Time Periodicity Rules (CRITICAL)

- A discrete-time exponential $x[n] = e^{j\omega_0 n}$ is periodic only if its frequency $\omega_0$ is a rational multiple of $2\pi$.
- That is, it is periodic if and only if:
  $$ \frac{\omega_0}{2\pi} = \frac{m}{N} $$
  where $m$ and $N$ are integers.
- This is a **major difference** from continuous time, where $e^{j\omega_0 t}$ is periodic for *any* $\omega_0$.



::: notes
This is one of the most important and sometimes tricky concepts. In discrete time, the signal is only defined at integer values of 'n'. For the signal to repeat, the total phase shift after N samples (which is omega_0 * N) must be a multiple of 2*pi. This condition only holds if the frequency is a rational multiple of 2*pi.
:::

## Discrete Periodicity Example

```{r dt_periodicity_plot, fig.height=5, fig.width=8}
n <- 0:40
omega1 <- pi / 8  # Periodic, since omega1/(2*pi) = 1/16
omega2 <- 1       # Not periodic, since 1/(2*pi) is irrational

periodic_signal <- cos(omega1 * n)
non_periodic_signal <- cos(omega2 * n)

periodic_data <- data.frame(n = n, value = periodic_signal, type = "Periodic: cos(pi*n/8)")
non_periodic_data <- data.frame(n = n, value = non_periodic_signal, type = "Non-Periodic: cos(n)")

combined_data <- rbind(periodic_data, non_periodic_data)

ggplot(combined_data, aes(x = n, y = value)) +
  geom_segment(aes(xend = n, yend = 0)) +
  geom_point() +
  facet_wrap(~type, nrow = 2, scales = "free_y") +
  labs(title = "Periodicity in Discrete-Time Cosine Sequences",
       x = "Sample (n)",
       y = "Amplitude") +
  theme_minimal()
```



::: notes
The top plot shows cos(pi*n/8). You can see a clear repeating pattern. The fundamental period is N=16, because after 16 samples, the phase has advanced by (pi/8)*16 = 2*pi. The bottom plot shows cos(n). Although the underlying continuous cosine is periodic, the samples of it are not. The pattern of sample values never exactly repeats, making the discrete-time signal non-periodic.
:::

## Frequency Aliasing in Discrete Time

- Another key difference in discrete time is that distinct frequencies can be **indistinguishable**.
- Specifically, the discrete-time complex exponentials with frequencies $\omega_0$ and $\omega_0 + 2\pi k$ for any integer $k$ are identical:
  $$ e^{j(\omega_0 + 2\pi k)n} = e^{j\omega_0 n} e^{j2\pi kn} = e^{j\omega_0 n} \cdot 1 = e^{j\omega_0 n} $$
- This means all the information about a discrete-time signal is contained in a frequency interval of length $2\pi$, for example, $[0, 2\pi)$ or $[-\pi, \pi)$.



::: notes
This is a profound concept. In continuous time, increasing the frequency omega always increases the rate of oscillation. In discrete time, this is not true. Once you go past pi, the perceived rate of oscillation starts to decrease. Frequencies like 3*pi/2 and -pi/2 are identical. This is called aliasing.
:::

## Visualizing Frequency Aliasing

```{r aliasing_plot, fig.height=5, fig.width=8}
n <- 0:15
omega0 <- pi / 4

# Two frequencies that are 2*pi apart
y1 <- cos(omega0 * n)
y2 <- cos((omega0 + 2*pi) * n)

# Create a data frame for plotting
plot_df <- data.frame(
  n = rep(n, 2),
  value = c(y1, y2),
  type = factor(rep(c("cos((pi/4) * n)", "cos((pi/4 + 2*pi) * n)"), each = length(n)))
)

# Plot
ggplot(plot_df, aes(x = n, y = value, color = type)) +
  geom_segment(aes(xend = n, yend = 0)) +
  geom_point(size=3) +
  labs(title = "Frequency Aliasing: Two Frequencies, One Signal",
       subtitle = "The samples for both signals are identical",
       x = "Sample (n)",
       y = "Amplitude") +
  theme_minimal() +
  theme(legend.title = element_blank())
```



::: notes
This plot demonstrates aliasing perfectly. We are plotting two discrete signals with very different underlying frequencies: pi/4 and pi/4 + 2*pi. However, when we sample them at integer values of 'n', the resulting sequences are identical. The points lie exactly on top of each other. This shows that in discrete time, these two frequencies are aliases of one another.
:::

# The Unit Impulse and Unit Step Functions

## The Unit Impulse Function

- The **unit impulse** (or unit sample) sequence, $\delta[n]$, is a fundamental building block.
  $$ \delta[n] = \begin{cases} 1, & n=0 \\ 0, & n \neq 0 \end{cases} $$

## The Unit Step Function

- The **unit step** sequence, $u[n]$, is defined as:
  $$ u[n] = \begin{cases} 1, & n \ge 0 \\ 0, & n < 0 \end{cases} $$
- They are related: $\delta[n] = u[n] - u[n-1]$ and, conversely, the unit step is the running sum of the impulse:
  $$ u[n] = \sum_{k=-\infty}^{n} \delta[k] $$



::: notes
The impulse and step functions are idealized signals that are incredibly important in system analysis. The impulse can be thought of as a single, instantaneous burst, while the step represents a signal that turns on at a specific time and stays on. Their relationship as a difference and a sum is a key concept.
:::

## Unit Step as a Sum of Impulses

```{r step_from_impulse_plot, fig.height=6, fig.width=8}
library(dplyr)
library(tidyr)

n_range <- -3:5

# Create data for 3 stages of summation
stage1_df <- data.frame(n = n_range, value = as.numeric(n_range == 0), stage = "Stage 1: Sum up to n=0 (δ[0])")
stage2_df <- data.frame(n = n_range, value = as.numeric(n_range >= 0 & n_range <= 2), stage = "Stage 2: Sum up to n=2 (δ[0]+δ[1]+δ[2])")
stage3_df <- data.frame(n = n_range, value = as.numeric(n_range >= 0), stage = "Stage 3: Full Sum (u[n])")

# Combine data
all_stages <- bind_rows(
  stage1_df %>% mutate(sum_val = cumsum(value)),
  stage2_df %>% mutate(sum_val = cumsum(value)),
  stage3_df %>% mutate(sum_val = cumsum(value))
) %>%
  mutate(stage = factor(stage, levels = unique(stage)))

# Plot
ggplot(all_stages, aes(x = n, y = sum_val)) +
  geom_segment(aes(xend = n, yend = 0), alpha = 0.7) +
  geom_point(size = 3) +
  facet_wrap(~stage, ncol = 1) +
  labs(title = "Visualizing the Unit Step u[n] as a Running Sum of Impulses",
       x = "Sample (n)",
       y = "Sum Value") +
  theme_minimal()
```



::: notes
This sequence of plots shows how the unit step is constructed by summing impulses. In Stage 1, we sum up to n=0, and we only capture the impulse at n=0. In Stage 2, we sum up to n=2, accumulating the impulses at 0, 1, and 2. In the final stage, the running sum has accumulated all impulses up to the current point, resulting in the full unit step sequence.
:::

## Continuous-Time Impulse and Step

- The continuous-time **unit step function**, $u(t)$, is defined as:
  $$ u(t) = \begin{cases} 1, & t > 0 \\ 0, & t < 0 \end{cases} $$
- The continuous-time **unit impulse function**, $\delta(t)$, is more abstract. It is considered to be the derivative of the unit step:
  $$ \delta(t) = \frac{du(t)}{dt} $$
- It is zero everywhere except at $t=0$, and its integral is one. $\int_{-\infty}^{\infty} \delta(t) dt = 1$.



::: notes
The continuous-time impulse is a "singularity function." It's not a function in the traditional sense. It's an idealization of a pulse that is infinitely short in duration but has a finite area of one. We can visualize it as the limit of a sequence of progressively shorter and taller pulses.
:::

## The Impulse as the Derivative of the Step

```{python step_derivative_plot, fig.height=5, fig.width=8}
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(t, k=50):
    """A steep sigmoid function to approximate the unit step."""
    return 1 / (1 + np.exp(-k * t))

def sigmoid_derivative(t, k=50):
    """The derivative of the sigmoid function."""
    sig_t = sigmoid(t, k)
    return k * sig_t * (1 - sig_t)

t = np.linspace(-0.5, 0.5, 1000)
approx_step = sigmoid(t)
approx_impulse = sigmoid_derivative(t)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

ax1.plot(t, approx_step)
ax1.set_title('Approximation of Unit Step $u(t)$')
ax1.grid(True)
ax1.set_xlabel('Time (t)')
ax1.set_ylabel('Amplitude')

ax2.plot(t, approx_impulse)
ax2.set_title('Derivative (Approximation of Impulse $\\delta(t)$)')
ax2.grid(True)
ax2.set_xlabel('Time (t)')

plt.suptitle('Derivative of a Step is an Impulse')
plt.show()
```



::: notes
Since the unit step has a discontinuity, its derivative is formally undefined. However, we can think of it by approximating the step with a smooth, steep function like the sigmoid shown on the left. The derivative of this steep function, shown on the right, is a narrow, tall pulse. As the sigmoid gets steeper and closer to an ideal step, its derivative gets narrower and taller, approaching our idealization of the unit impulse.
:::

## The Sifting Property of the Impulse

- One of the most important properties of the unit impulse is the **sifting property**.
- Multiplying a signal by a shifted impulse isolates the signal's value at that point.
- **Discrete-Time:**
  $$ x[n]\delta[n-n_0] = x[n_0]\delta[n-n_0] $$
- **Continuous-Time:**
  $$ x(t)\delta(t-t_0) = x(t_0)\delta(t-t_0) $$
- This property is fundamental to the theory of sampling and system representation.



::: notes
The term "sifting" is very descriptive. Think of the impulse as a sieve that only lets the value of the signal at one specific point pass through. All other values are multiplied by zero and eliminated. We will use this property extensively when we discuss system responses.
:::

## Visualizing the Sifting Property

```{r sifting_plot, fig.height=6, fig.width=8}
# Define a discrete signal
n <- -2:8
x_n <- sin(0.5 * n) * exp(-0.1 * n)
signal_df <- data.frame(n = n, value = x_n, type = "Signal x[n]")

# Define an impulse
n0 <- 3
impulse_df <- data.frame(n = n, value = as.numeric(n == n0), type = "Impulse δ[n-3]")

# Multiply them
product_df <- data.frame(n = n, value = x_n * as.numeric(n == n0), type = "Product x[n]δ[n-3]")

# Combine for plotting
plot_df <- bind_rows(signal_df, impulse_df, product_df) %>%
  mutate(type = factor(type, levels = c("Signal x[n]", "Impulse δ[n-3]", "Product x[n]δ[n-3]")))

# Plot
ggplot(plot_df, aes(x = n, y = value)) +
  geom_segment(aes(xend = n, yend = 0)) +
  geom_point(size = 3) +
  facet_wrap(~type, ncol = 1) +
  labs(title = "The Sifting Property",
       x = "Sample (n)",
       y = "Amplitude") +
  theme_minimal()
```



::: notes
The top plot shows an arbitrary signal x[n]. The middle plot shows a unit impulse located at n=3. When we multiply these two signals point-by-point, the result is zero everywhere except at n=3, where the impulse is non-zero. The value of the resulting signal at n=3 is exactly the value of the original signal x[n] at n=3.
:::

# Systems

## Memoryless Systems

- A system is **memoryless** if its output at any given time depends only on the input at that same time.
  - Example: A resistor, $v(t) = R \cdot i(t)$. The voltage now depends on the current now.

## Systems with Memory

- A system has **memory** if its output depends on past (or future) values of the input.
  - Example: A capacitor, $v(t) = \frac{1}{C}\int_{-\infty}^{t} i(\tau)d\tau$. The voltage now depends on all past current.



::: notes
The concept of memory is about cause and effect. In a memoryless system, the effect is simultaneous with the cause. In a system with memory, past inputs influence the present output. This is often associated with the system's ability to store energy, like a capacitor storing charge or a mass storing kinetic energy.
:::

## Memory vs. Memoryless Systems

```{python memory_plot, fig.height=5, fig.width=8}
import numpy as np
import matplotlib.pyplot as plt

# --- Resistor (Memoryless) ---
# Input current: a simple sine wave
i_t = np.linspace(-5, 5, 1000)
R = 2.0
v_resistor = R * i_t # v(t) = R*i(t)

# --- Capacitor (With Memory) ---
# Input current: a sine wave over time
t = np.linspace(0, 4*np.pi, 1000)
i_cap_t = np.sin(t)
C = 1.0
v_capacitor = (1/C) * np.cumsum(i_cap_t) * (t[1]-t[0]) # Integral of current

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# Plot Resistor (output vs input)
ax1.plot(i_t, v_resistor)
ax1.set_title('Memoryless System: Resistor')
ax1.set_xlabel('Input Current i(t)')
ax1.set_ylabel('Output Voltage v(t)')
ax1.grid(True)

# Plot Capacitor (output vs input)
ax2.plot(i_cap_t, v_capacitor)
ax2.set_title('System with Memory: Capacitor')
ax2.set_xlabel('Input Current i(t)')
ax2.set_ylabel('Output Voltage v(t)')
ax2.grid(True)

plt.suptitle('Input vs. Output Behavior')
plt.show()
```



::: notes
The plot on the left shows the input-output relationship for a resistor. It's a straight line, meaning the output voltage is always directly proportional to the input current at that instant. This is a memoryless system. The plot on the right shows the relationship for a capacitor with a sinusoidal input current. It forms a loop, known as a hysteresis loop. For a given input current, the output voltage can be different depending on the history of the input, demonstrating the system's memory.
:::

## Invertibility and Inverse Systems

- A system is **invertible** if distinct inputs produce distinct outputs.
- If a system is invertible, an **inverse system** exists which, when cascaded with the original system, produces the original input.
- Example: The system $y(t) = 2x(t)$ is invertible. Its inverse is $w(t) = \frac{1}{2}y(t)$.
- Example: The system $y(t) = x^2(t)$ is **not** invertible, because both $x(t)=2$ and $x(t)=-2$ produce the same output $y(t)=4$. We cannot uniquely determine the input from the output.



::: notes
Invertibility is crucial in applications like communications and data compression. If you apply an encoding or compression scheme to a signal, you need to be able to reverse the process to get the original signal back. This means the encoding system must be invertible.
:::

## Visualizing Invertibility

```{r invertibility_plot, fig.height=5, fig.width=8}
x <- seq(-3, 3, length.out = 500)
y_invertible <- 2 * x
y_non_invertible <- x^2

invertible_df <- data.frame(Input = x, Output = y_invertible, Type = "Invertible System: y = 2x")
non_invertible_df <- data.frame(Input = x, Output = y_non_invertible, Type = "Non-Invertible System: y = x^2")

combined_df <- rbind(invertible_df, non_invertible_df)

ggplot(combined_df, aes(x = Input, y = Output)) +
  geom_line() +
  facet_wrap(~Type, scales = "free") +
  labs(title = "Invertible vs. Non-Invertible Systems",
       x = "Input x",
       y = "Output y") +
  theme_minimal() +
  geom_hline(data = data.frame(yintercept=4, Type="Non-Invertible System: y = x^2"), aes(yintercept=yintercept), linetype="dashed", color="red") +
  geom_vline(data = data.frame(xintercept=c(-2, 2), Type="Non-Invertible System: y = x^2"), aes(xintercept=xintercept), linetype="dotted", color="red")
```



::: notes
The plot on the left shows a clear, one-to-one mapping between input and output. For any given output value, there is only one possible input value. This system is invertible. The plot on the right shows the relationship y=x^2. Notice that for an output value of y=4 (the dashed red line), there are two possible input values, x=-2 and x=2. Since we can't uniquely determine the input, this system is non-invertible.
:::

## Causality

- A system is **causal** if its output at any time depends only on the present and past values of the input.
- In other words, a causal system does not "anticipate" future inputs. The output $y(t_0)$ can only depend on $x(t)$ for $t \le t_0$.
- All real-time physical systems are causal.
- Non-causal systems can be built for applications where the input signal is pre-recorded (e.g., image processing, stock market analysis).
- Example of non-causal: $y[n] = x[n+1]$ (output depends on the future).



::: notes
Causality is a fundamental constraint in systems that operate in real time. You can't react to an event before it happens. However, if you have a complete recording of a signal, you can design a non-causal filter. For example, to smooth a noisy signal at a certain point, you might want to average the values just before and just after that point, which requires knowledge of "future" data.
:::

## Causal vs. Non-Causal Filtering

```{python causality_plot, fig.height=5, fig.width=8}
import numpy as np
import matplotlib.pyplot as plt
from scipy.signal import lfilter

# Create a noisy step signal
n = np.arange(100)
x_n = (n >= 30).astype(float) + np.random.normal(0, 0.2, 100)

# Non-causal filter (centered moving average)
# This uses future values (e.g., at n=50, it uses n=51, 52...)
window_size = 5
y_non_causal = np.convolve(x_n, np.ones(window_size)/window_size, mode='same')

# Causal filter (simple recursive filter, e.g., y[n] = 0.9y[n-1] + 0.1x[n])
b = [0.1]
a = [1, -0.9]
y_causal = lfilter(b, a, x_n)


plt.figure(figsize=(12, 6))
plt.plot(n, x_n, 'k.', alpha=0.5, label='Original Noisy Signal')
plt.plot(n, y_non_causal, 'b-', label='Non-Causal Smoothed Signal')
plt.plot(n, y_causal, 'r-', label='Causal Smoothed Signal')
plt.title('Causal vs. Non-Causal Smoothing Filters')
plt.xlabel('Sample (n)')
plt.ylabel('Amplitude')
plt.legend()
plt.grid(True)
plt.show()
```



::: notes
The black dots represent a noisy step input. The blue line is the output of a non-causal smoothing filter. Notice how it starts to rise *before* the step at n=30. This is because it averages future values, anticipating the jump. The red line is from a causal filter. It only starts to react at n=30 and exhibits a characteristic lag as it catches up to the input. This lag is typical of real-time causal systems.
:::

## Stability

- **Stability** is a critical property for most systems. Informally, a stable system is one where small inputs lead to responses that do not diverge.
- We use the **Bounded-Input, Bounded-Output (BIBO)** definition:
  > A system is stable if every bounded input produces a bounded output.

## BIBO Stability Conditions

- **Bounded Input:** There exists a constant $B$ such that $|x(t)| \le B$ for all $t$.
- **Bounded Output:** There exists a constant $C$ such that $|y(t)| \le C$ for all $t$.
- Example of an unstable system: an accumulator. $y[n] = \sum_{k=-\infty}^{n} x[k]$. A bounded step input $u[n]$ leads to an unbounded ramp output.



::: notes
Stability is about predictability and control. If you put a bounded, well-behaved signal into a system, you expect the output to also be well-behaved. If it can "explode" or grow without limit, the system is unstable and often dangerous in physical applications. Think of the inverted pendulum vs. the stable one.
:::

## Stable vs. Unstable System Response

```{r stability_plot, fig.height=5, fig.width=8}
# Define a discrete system function for simulation
simulate_system <- function(x, a) {
  y <- numeric(length(x))
  for (n in 2:length(x)) {
    y[n] <- a * y[n-1] + x[n]
  }
  return(y)
}

# Input signal: a unit step
n <- 0:50
x_n <- as.numeric(n >= 5)

# Stable system (pole |a| < 1)
y_stable <- simulate_system(x_n, a = 0.9)
stable_df <- data.frame(n = n, input = x_n, output = y_stable, type = "Stable System (y[n]=0.9y[n-1]+x[n])")

# Unstable system (pole |a| > 1)
y_unstable <- simulate_system(x_n, a = 1.05)
unstable_df <- data.frame(n = n, input = x_n, output = y_unstable, type = "Unstable System (y[n]=1.05y[n-1]+x[n])")

combined_df <- rbind(stable_df, unstable_df)

ggplot(combined_df, aes(x = n)) +
  geom_line(aes(y = input, color = "Input"), linetype = "dashed") +
  geom_line(aes(y = output, color = "Output")) +
  facet_wrap(~type, scales = "free_y") +
  labs(title = "Response of Stable and Unstable Systems to a Step Input",
       x = "Sample (n)", y = "Amplitude") +
  scale_color_manual(name = "Signal", values = c("Input" = "black", "Output" = "blue")) +
  theme_minimal()
```



::: notes
Both systems are given the same bounded step input. The stable system on the left has a response that rises and settles to a finite, bounded value. The unstable system on the right, however, has a response that grows exponentially without limit. Even though the input is perfectly bounded, the output diverges. This is the hallmark of an unstable system.
:::

## Linearity

- A system is **linear** if it possesses the property of **superposition**.
- **Superposition** = Additivity + Homogeneity
- Combined: $ax_1(t) + bx_2(t) \to ay_1(t) + by_2(t)$.

## Linearity Properties

- **Additivity:** If $x_1(t) \to y_1(t)$ and $x_2(t) \to y_2(t)$, then $x_1(t) + x_2(t) \to y_1(t) + y_2(t)$.
- **Homogeneity:** If $x(t) \to y(t)$, then $ax(t) \to ay(t)$ for any complex constant $a$.



::: notes
Linearity is arguably the most important property we will study. Linear systems are much easier to analyze than non-linear ones, and many real-world systems can be approximated as linear. The superposition principle allows us to break down complex inputs into simpler components, analyze the system's response to each, and then add the responses up.
:::

## Time-Invariance

- A system is **time-invariant** if its behavior does not depend on what time it is.
- Mathematically: If $x(t) \to y(t)$, then $x(t-t_0) \to y(t-t_0)$ for any time shift $t_0$.
- A time-shift in the input signal causes an identical time-shift in the output signal.
- An RC circuit with constant R and C values is time-invariant. If R or C varied with time, the system would be time-varying.



::: notes
Time-invariance means the "rules" of the system don't change over time. If you perform an experiment today and get a certain result, you should get the same result if you perform the exact same experiment tomorrow. The combination of Linearity and Time-Invariance (LTI) defines the class of systems we will focus on for most of this course.
:::

# Chapter 1 Summary

- **Signals** are functions that carry information. They can be continuous-time (CT) or discrete-time (DT).
- We can transform signals by **shifting**, **scaling**, or **reversing** the independent variable.
- **Complex exponentials** and **sinusoids** are fundamental building blocks for many other signals.
- **Unit impulses** and **unit steps** are idealized signals used for system analysis.
- **Systems** transform input signals into output signals and are characterized by properties like **memory**, **invertibility**, **causality**, **stability**, **linearity**, and **time-invariance**.



::: notes
This chapter has laid the mathematical groundwork for everything that follows. We've introduced the language of signals and systems. The key takeaway is the classification of signals and the properties of systems. In the next chapter, we will focus heavily on the class of Linear Time-Invariant (LTI) systems.
:::
