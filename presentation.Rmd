---
title: "Chapter 1: Signals and Systems"
output: powerpoint_presentation
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
library(dplyr)
library(patchwork)
library(reticulate)
library(plotly)

# Use the system's python3
use_python("/usr/bin/python3", required = TRUE)
```

# Part 1: Deep Dive into Signals

## Slide 1: Introduction to Signals
- **What is a signal?**
  - Information conveyed through a pattern of variations.
  - Examples:
    - Electrical signals (voltage, current)
    - Audio signals (acoustic pressure)
    - Images (brightness, color)
- We represent signals as functions of one or more independent variables.
- This chapter focuses on signals with a **single independent variable: time (t)**.

::: notes
Welcome. Today we begin our study of Signals and Systems. At its core, a signal is simply a way to convey information. This information is encoded in a pattern. Think of the voltage in a circuit, the sound of my voice, or the brightness in a picture. These are all signals. We will use mathematics to describe these patterns, treating them as functions of an independent variable, which for most of our work, will be time.
:::

## Slide 2: Continuous-Time (CT) vs. Discrete-Time (DT) Signals

- **Continuous-Time (CT) Signals:**
  - Defined for a continuous range of time values.
  - The independent variable `t` is continuous.
  - Notation: `x(t)`
- **Discrete-Time (DT) Signals:**
  - Defined only at specific, discrete points in time.
  - The independent variable `n` is an integer.
  - Notation: `x[n]`
  - Often arise from *sampling* a continuous-time signal.

::: notes
We will study two fundamental types of signals. Continuous-time signals, denoted x(t), exist at every moment in time. Think of the smooth flow of a river. Discrete-time signals, x[n], are like snapshots taken at specific intervals. Much of digital technology, from audio processing to control systems, relies on converting continuous signals into discrete ones through a process called sampling.
:::

## Slide 3: Visualizing a CT Signal: Dow Jones Index
This plot shows the weekly Dow Jones stock market index from January 5, 1929, to January 4, 1930. It is a real-world example of a signal where the value (stock index) changes over the independent variable (time).

```{r dow_jones, echo=FALSE, fig.width=8, fig.height=4}
# Data extracted from Figure 1.6 in chapter1.md
time <- seq(as.Date("1929-01-05"), as.Date("1930-01-04"), by="weeks")
# Approximate values eyeballed from the graph
dow_values <- c(300, 310, 305, 320, 315, 310, 325, 330, 340, 350, 345, 330, 320, 310, 300, 290, 280, 295, 305, 320, 330, 355, 350, 340, 330, 350, 370, 380, 375, 360, 350, 320, 290, 260, 230, 200, 220, 240, 250, 260, 255, 250, 245, 240, 250, 260, 270, 265, 260, 250, 245, 255)

# Ensure the data has the same length as time
dow_data <- data.frame(Date = time[1:length(dow_values)], Index = dow_values)

ggplot(dow_data, aes(x = Date, y = Index)) +
  geom_line() +
  labs(title = "Dow Jones Industrial Average (Weekly)",
       subtitle = "Jan 5, 1929 to Jan 4, 1930",
       x = "Date",
       y = "Index Value") +
  theme_minimal()
```

::: notes
Here is a classic example of a signal from the world of finance. This is the Dow Jones index over a tumultuous year in market history. We can see the dramatic crash of 1929 in this data. This graph is a continuous-time representation, where each point in time has a corresponding index value.
:::

## Slide 4: The Concept of Sampling
- Sampling is the process of converting a continuous-time signal into a discrete-time signal.
- We measure the value of the CT signal `x(t)` at regular intervals, `t = nT`, where `T` is the sampling period.
- The resulting discrete-time signal is `x[n] = x(nT)`.

::: notes
How do we move from the continuous world to the discrete world? Through sampling. Imagine you have a video of a moving car. Taking snapshots at a fixed rate, say every second, is sampling. The sequence of snapshots is your discrete-time signal. The time between snapshots is the sampling period.
:::

## Slide 5: Visualizing Sampling (1/3): The Original CT Signal
Here is a continuous sine wave, `x(t) = sin(t)`. It is defined for all values of `t`.

```{r sampling1, echo=FALSE, fig.width=8, fig.height=4}
t <- seq(0, 2*pi, length.out = 500)
df_cont <- data.frame(t = t, y = sin(t))
ggplot(df_cont, aes(x = t, y = y)) +
  geom_line() +
  labs(title = "Continuous Sine Wave: x(t) = sin(t)", x = "Time (t)", y = "Amplitude") +
  theme_minimal()
```

::: notes
Let's start with a pure, continuous signal: a simple sine wave. It's smooth, and it exists at every single point in time. This is our "analog" reality before we digitize it.
:::

## Slide 6: Visualizing Sampling (2/3): Overlaying Sample Points
Now, we sample the sine wave at regular intervals. The red dots represent the measured values.

```{r sampling2, echo=FALSE, fig.width=8, fig.height=4}
sample_rate <- 8
n <- 0:16
df_disc <- data.frame(t = n * (2*pi/sample_rate), y = sin(n * (2*pi/sample_rate)))

ggplot(df_cont, aes(x = t, y = y)) +
  geom_line(color="lightblue") +
  geom_point(data = df_disc, color="red", size=3) +
  labs(title = "Sampling the Sine Wave", x = "Time (t)", y = "Amplitude") +
  theme_minimal()
```

::: notes
Here, we overlay our sampling process onto the continuous wave. The red dots are the only information our digital system will capture. We are taking snapshots at fixed intervals.
:::

## Slide 7: Visualizing Sampling (3/3): The Resulting DT Signal
This is the discrete-time signal `x[n]` that results from the sampling. Information *between* the sample points is lost.

```{r sampling3, echo=FALSE, fig.width=8, fig.height=4}
ggplot(df_disc, aes(x = t, y = y)) +
  geom_segment(aes(xend=t, yend=0)) +
  geom_point() +
  labs(title = "Discrete-Time Stem Plot from Sampling", x = "Time (nT)", y = "Amplitude") +
  theme_minimal()
```
::: notes
This is what our computer sees. Not the smooth curve, but a sequence of values, or samples. Notice the critical point: we have no information about what happened between these samples. This loss of information is a fundamental trade-off in digital signal processing.
:::

## Slide 8: Signal Energy and Power
- We often use concepts of "energy" and "power" to characterize signals, motivated by physical systems.
- **Total Energy** over all time ($$E_{\infty}$$):
  - CT: $$E_{\infty} = \int_{-\infty}^{\infty} |x(t)|^2 dt$$
  - DT: $$E_{\infty} = \sum_{n=-\infty}^{\infty} |x[n]|^2$$
- **Average Power** over all time ($$P_{\infty}$$):
  - CT: $$P_{\infty} = \lim_{T \to \infty} \frac{1}{2T} \int_{-T}^{T} |x(t)|^2 dt$$
  - DT: $$P_{\infty} = \lim_{N \to \infty} \frac{1}{2N+1} \sum_{n=-N}^{N} |x[n]|^2$$

::: notes
To quantify the "size" or "strength" of a signal, we borrow the concepts of energy and power from physics. The total energy is the sum or integral of the squared magnitude over all time. The average power is the time-average of this quantity. A signal can have finite energy, finite power, or neither.
:::

## Slide 9: Finite Energy vs. Finite Power Signals
- **Energy Signals:**
  - Have finite total energy ($$E_{\infty} < \infty$$).
  - Must have zero average power ($$P_{\infty} = 0$$).
  - These are signals that are time-limited or decay to zero.
- **Power Signals:**
  - Have infinite total energy ($$E_{\infty} = \infty$$).
  - But have finite average power ($$0 < P_{\infty} < \infty$$).
  - Periodic signals (like sine waves) are a key example.

::: notes
This leads to a useful classification. "Energy signals" are transient; they exist for a finite duration or die out. Think of a clap of thunder. "Power signals" are persistent; they go on forever with a steady strength. Think of the 60 Hz hum from an electrical outlet. An energy signal has finite energy and thus zero average power. A power signal has infinite energy but a finite, non-zero average power.
:::

## Slide 10: Example: Energy of a Decaying Exponential
Let's analyze the signal $$v(t) = 5e^{-0.2t}u(t)$$, where $$u(t)$$ is the unit step function.

- This is an **energy signal**. Its amplitude decays over time.
- **Instantaneous Power** across a 1Ω resistor would be $$p(t) = v(t)^2 = 25e^{-0.4t}u(t)$$.
- **Total Energy** is the integral of the instantaneous power.

::: notes
Let's consider a practical example. A capacitor discharging through a resistor produces a decaying exponential voltage. This is a classic energy signal because its amplitude, and therefore its ability to deliver power, diminishes over time until it's gone. We will now visualize how its power and energy evolve.
:::

## Slide 11: Visualizing Instantaneous Power and Cumulative Energy
```{python energy_power_plot, echo=FALSE}
import numpy as np
import matplotlib.pyplot as plt

t = np.linspace(0, 20, 500)
v = 5 * np.exp(-0.2 * t)
p = v**2
E = 25 / 0.4 * (1 - np.exp(-0.4 * t))

fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 6), sharex=True)

ax1.plot(t, p)
ax1.set_title("Instantaneous Power p(t)")
ax1.set_ylabel("Power (W)")
ax1.grid(True)

ax2.plot(t, E)
ax2.set_title("Cumulative Energy E(t)")
ax2.set_ylabel("Energy (J)")
ax2.set_xlabel("Time (t)")
ax2.axhline(y=25/0.4, color='r', linestyle='--', label='Total Energy E_inf = 62.5 J')
ax2.legend()
ax2.grid(True)

plt.tight_layout()
plt.show()
```

::: notes
The top plot shows the instantaneous power, p(t). It starts high and decays to zero. The bottom plot shows the cumulative energy, E(t), which is the integral of the power. Notice how the energy accumulates over time and settles at a final, finite value, which is the total energy of the signal. This confirms it is an energy signal.
:::

## Slide 12: Summary of Part 1
- Signals carry information in patterns.
- We distinguish between Continuous-Time `x(t)` and Discrete-Time `x[n]` signals.
- **Sampling** converts CT signals to DT signals, with an inherent loss of information.
- **Energy** and **Power** are metrics to classify signals.
  - **Energy Signals:** Finite energy, decay over time.
  - **Power Signals:** Finite power, persist over time (e.g., periodic signals).

::: notes
To summarize Part 1, we have established a basic language for discussing signals. We know the difference between continuous and discrete time, we understand the critical process of sampling, and we can classify signals based on their energy and power content. Now, we will explore how we can manipulate and transform these signals.
:::

# Part 2: Dynamic Transformations

## Slide 13: Transformations of the Independent Variable
- We can transform a signal by manipulating the time variable.
- Three fundamental transformations:
  1. **Time Shifting:** `y(t) = x(t - t_0)`
  2. **Time Scaling:** `y(t) = x(at)`
  3. **Time Reversal:** `y(t) = x(-t)` (a special case of scaling)
- These operations are essential for modeling and analyzing real-world systems like radar, sonar, and audio playback.

::: notes
In the second part of our lecture, we'll look at what happens when we manipulate the time axis itself. We can shift time, scale it, or even reverse it. These are not just mathematical curiosities; they model real physical phenomena. Think about the delay in a satellite communication—that's a time shift. Or playing a song at double speed—that's time scaling.
:::

## Slide 14: Time Shifting
- A time shift is represented by `y(t) = x(t - t_0)`.
- If `t_0 > 0`, the signal is **delayed** (shifted to the right).
- If `t_0 < 0`, the signal is **advanced** (shifted to the left).

::: notes
Let's focus on time shifting. The notation is `x(t - t_0)`. The key is the sign of `t_0`. A positive `t_0` means a delay—the signal happens later. A negative `t_0` means an advance—the signal happens earlier. Let's visualize this with a simple pulse.
:::

## Slide 15: Visualizing Time Shifting (1/3): Original Pulse
Here is a rectangular pulse signal `x(t)` centered at the origin.

```{r time_shift1, echo=FALSE, fig.width=8, fig.height=4}
pulse <- function(t) {
  ifelse(t >= -1 & t <= 1, 1, 0)
}
df_pulse <- data.frame(t = seq(-4, 4, length.out=500))
df_pulse$y <- pulse(df_pulse$t)

ggplot(df_pulse, aes(x=t, y=y)) + geom_line(size=1) +
  labs(title="Original Pulse Signal: x(t)", x="t", y="Amplitude") +
  ylim(0, 1.2) + theme_minimal()
```

::: notes
This is our reference signal, a simple pulse `x(t)`. It's centered at time t=0 and has a width of 2.
:::

## Slide 16: Visualizing Time Shifting (2/3): Delayed Pulse
This is `x(t - 2)`. The pulse is delayed by 2 units, now centered at `t = 2`.

```{r time_shift2, echo=FALSE, fig.width=8, fig.height=4}
df_pulse$y_delayed <- pulse(df_pulse$t - 2)
ggplot(df_pulse, aes(x=t, y=y_delayed)) + geom_line(size=1, color="blue") +
  labs(title="Delayed Pulse: x(t - 2)", x="t", y="Amplitude") +
  ylim(0, 1.2) + theme_minimal()
```

::: notes
Now we look at `x(t - 2)`. Here, `t_0` is +2, so it's a delay. The entire pulse has shifted to the right by 2 units.
:::

## Slide 17: Visualizing Time Shifting (3/3): Advanced Pulse
This is `x(t + 2)`, which is `x(t - (-2))`. The pulse is advanced by 2 units, now centered at `t = -2`.

```{r time_shift3, echo=FALSE, fig.width=8, fig.height=4}
df_pulse$y_advanced <- pulse(df_pulse$t + 2)
ggplot(df_pulse, aes(x=t, y=y_advanced)) + geom_line(size=1, color="red") +
  labs(title="Advanced Pulse: x(t + 2)", x="t", y="Amplitude") +
  ylim(0, 1.2) + theme_minimal()
```

::: notes
Finally, `x(t + 2)`. This corresponds to a `t_0` of -2, so it's an advance. The pulse has shifted to the left by 2 units. This simple sequence illustrates the core idea of time shifting.
:::

## Slide 18: Time Scaling
- Time scaling is represented by `y(t) = x(at)`.
- If `|a| > 1`, the signal is **compressed** in time.
- If `0 < |a| < 1`, the signal is **expanded** in time.
- If `a < 0`, the signal is also **time-reversed**.

::: notes
Next, let's consider time scaling, `x(at)`. The magnitude of `a` determines whether the signal is compressed or expanded. If `a` is greater than 1, the signal happens faster—it's compressed. If `a` is between 0 and 1, the signal happens slower—it's expanded.
:::

## Slide 19: Visualizing Time Scaling
Comparing `x(t)` (a triangular pulse), `x(2t)` (compressed), and `x(0.5t)` (expanded).

```{python time_scaling_plot, echo=FALSE}
import numpy as np
import matplotlib.pyplot as plt

def triangular_pulse(t):
    return np.maximum(0, 1 - np.abs(t))

t = np.linspace(-4, 4, 1000)
x_t = triangular_pulse(t)
x_2t = triangular_pulse(2 * t)
x_05t = triangular_pulse(0.5 * t)

plt.figure(figsize=(10, 4))
plt.plot(t, x_t, label='x(t)')
plt.plot(t, x_2t, label='x(2t) [Compressed]', linestyle='--')
plt.plot(t, x_05t, label='x(0.5t) [Expanded]', linestyle=':')
plt.title("Time Scaling of a Triangular Pulse")
plt.xlabel("Time (t)")
plt.ylabel("Amplitude")
plt.grid(True)
plt.axhline(0, color='black', linewidth=0.5)
plt.axvline(0, color='black', linewidth=0.5)

# Mark zero-crossings
plt.scatter([-1, 1], [0, 0], color='blue', zorder=5) # x(t)
plt.scatter([-0.5, 0.5], [0, 0], color='red', zorder=5) # x(2t)
plt.scatter([-2, 2], [0, 0], color='green', zorder=5) # x(0.5t)

plt.legend()
plt.show()
```

::: notes
This plot clearly shows the effect. The blue line is our original signal, `x(t)`, which goes from -1 to 1. The orange dashed line is `x(2t)`. Notice it's twice as fast and only spans from -0.5 to 0.5. The green dotted line is `x(0.5t)`. It's half the speed and is expanded to span from -2 to 2. The zero-crossings, marked with dots, confirm this compression and expansion.
:::


## Slide 20: Even and Odd Signals
- A signal can be decomposed into its even and odd components.
- **Even Signal:** `x_e(t) = x_e(-t)` (Symmetric around the vertical axis)
- **Odd Signal:** `x_o(t) = -x_o(-t)` (Anti-symmetric around the origin)

Any signal `x(t)` can be written as `x(t) = x_e(t) + x_o(t)`.

::: notes
Another important way to classify signals is by their symmetry. An even signal is a mirror image of itself around the t=0 axis, like the cosine function. An odd signal is symmetric through the origin, like the sine function. What's powerful is that *any* signal, no matter how complex, can be broken down into a sum of a purely even part and a purely odd part.
:::

## Slide 21: Even/Odd Decomposition Formulas
- The **even part** of `x(t)` is calculated as:
  $$x_e(t) = \frac{1}{2} [x(t) + x(-t)]$$
- The **odd part** of `x(t)` is calculated as:
  $$x_o(t) = \frac{1}{2} [x(t) - x(-t)]$$

- The same formulas apply to discrete-time signals `x[n]`.

::: notes
These are the formulas that perform the decomposition. To get the even part, you add the signal to its time-reversed version and divide by two. This reinforces the symmetric components and cancels the anti-symmetric ones. To get the odd part, you subtract the time-reversed version from the original signal and divide by two. This cancels the symmetric parts and reinforces the anti-symmetric ones.
:::

## Slide 22: Visualizing Decomposition (1/3): Original DT Signal
Here is an arbitrary discrete-time sequence `x[n]`.

```{r even_odd1, echo=FALSE, fig.width=8, fig.height=4}
n_vals <- -5:5
x_n <- c(0, 0, 2, 3, 1, -1, -2, 0, 1, 0, 0)
df_eo <- data.frame(n = n_vals, x = x_n)

ggplot(df_eo, aes(x=n, y=x)) +
  geom_segment(aes(xend=n, yend=0)) +
  geom_point() +
  labs(title="Original DT Signal x[n]", x="n", y="Value") +
  theme_minimal()
```

::: notes
Let's visualize this decomposition. We start with this arbitrary sequence of numbers, `x[n]`. It doesn't appear to have any obvious symmetry.
:::

## Slide 23: Visualizing Decomposition (2/3): The Even Component
This is the even component, `x_e[n] = 0.5 * (x[n] + x[-n])`. Note the symmetry around `n=0`.

```{r even_odd2, echo=FALSE, fig.width=8, fig.height=4}
# To compute x[-n], we need to be careful with the indexing
# x_n corresponds to n_vals from -5 to 5.
# x[-n] means we need the value of x at -n.
# e.g., for n=2, we need x[-2], which is at index 4 in x_n
# e.g., for n=-2, we need x[2], which is at index 8 in x_n
x_neg_n_vals <- sapply(df_eo$n, function(n_val) {
  # Find the value of x at -n_val
  if (-n_val %in% df_eo$n) {
    df_eo$x[df_eo$n == -n_val]
  } else {
    0
  }
})
df_eo$xe <- 0.5 * (df_eo$x + x_neg_n_vals)

ggplot(df_eo, aes(x=n, y=xe)) +
  geom_segment(aes(xend=n, yend=0)) +
  geom_point() +
  labs(title="Even Component xe[n]", x="n", y="Value") +
  theme_minimal()
```

::: notes
Applying the formula, we get the even part of our signal. As you can see, it is perfectly symmetric around the vertical axis.
:::

## Slide 24: Visualizing Decomposition (3/3): The Odd Component
This is the odd component, `x_o[n] = 0.5 * (x[n] - x[-n])`. Note the anti-symmetry.

```{r even_odd3, echo=FALSE, fig.width=8, fig.height=4}
# We can reuse the x_neg_n_vals from the previous chunk
x_neg_n_vals <- sapply(df_eo$n, function(n_val) {
  if (-n_val %in% df_eo$n) {
    df_eo$x[df_eo$n == -n_val]
  } else {
    0
  }
})
df_eo$xo <- 0.5 * (df_eo$x - x_neg_n_vals)

ggplot(df_eo, aes(x=n, y=xo)) +
  geom_segment(aes(xend=n, yend=0)) +
  geom_point() +
  labs(title="Odd Component xo[n]", x="n", y="Value") +
  theme_minimal()
```

::: notes
And here is the odd part. Notice how the value at n=2 is the negative of the value at n=-2, and so on. This is the definition of odd symmetry. If you were to add this odd component to the even component from the previous slide, you would perfectly reconstruct the original signal.
:::

## Slide 25: Summary of Part 2
- **Time Shifting:** `x(t - t_0)` delays or advances a signal.
- **Time Scaling:** `x(at)` compresses or expands a signal.
- **Even/Odd Decomposition:** Any signal can be split into a symmetric (even) part and an anti-symmetric (odd) part.
  - $$x_e(t) = \frac{1}{2} [x(t) + x(-t)]$$
  - $$x_o(t) = \frac{1}{2} [x(t) - x(-t)]$$
- These transformations are fundamental tools for signal analysis.

::: notes
So, in this section, we've learned how to shift, scale, and reflect signals in time. We also learned about a powerful analytical tool: decomposing a signal into its even and odd components. These operations form the basis for understanding more complex signal processing techniques.
:::

# Part 3: The Complex Exponential Family

## Slide 26: The Exponential Signal
- The continuous-time exponential signal is of the form:
  $$x(t) = Ce^{at}$$
- `C` and `a` are, in general, complex numbers.
- This family of signals is of paramount importance in signals and systems. They are the basic building blocks for many other signals.

::: notes
We now turn to what is arguably the most important family of signals in this entire field: the exponential signals. The form is simple, `Ce^(at)`. But by allowing C and 'a' to be complex numbers, we unlock a rich set of behaviors that can describe a vast range of physical phenomena.
:::

## Slide 27: Real Exponentials
- When `C` and `a` are real numbers.
- If `a > 0`, the signal **grows** exponentially.
  - Models chain reactions, population growth.
- If `a < 0`, the signal **decays** exponentially.
  - Models radioactive decay, RC circuit responses.
- If `a = 0`, the signal is a **constant**.

::: notes
Let's start with the simplest case where C and 'a' are real. If 'a' is positive, we get exponential growth. If 'a' is negative, we get exponential decay, a behavior we've already seen. If 'a' is zero, we just have a constant DC signal.
:::

## Slide 28: The "Time Constant" Concept (τ)
- For a decaying exponential, we often write `a = -1/τ`.
  $$x(t) = Ce^{-t/\tau}$$
- **τ (tau)** is the **time constant**.
- It represents the time it takes for the signal to decay to approximately 37% (`1/e`) of its initial value.
- A **smaller τ** means **faster decay**.
- A **larger τ** means **slower decay**.

::: notes
For decaying exponentials, the concept of the time constant, tau, is extremely useful. It gives us a standardized way to talk about the rate of decay. After one time constant has passed, the signal has decayed to about 37% of its starting value. A small tau means a very fast decay, while a large tau means a long, slow decay.
:::

## Slide 29: Visualizing the Time Constant
```{python time_constant_plot, echo=FALSE}
import numpy as np
import matplotlib.pyplot as plt

t = np.linspace(0, 10, 500)
tau_05 = 0.5
tau_1 = 1.0
tau_5 = 5.0

x_05 = np.exp(-t / tau_05)
x_1 = np.exp(-t / tau_1)
x_5 = np.exp(-t / tau_5)

plt.figure(figsize=(10, 5))
plt.plot(t, x_05, label=f'τ = {tau_05} (Fastest Decay)')
plt.plot(t, x_1, label=f'τ = {tau_1}')
plt.plot(t, x_5, label=f'τ = {tau_5} (Slowest Decay)')

plt.title("The Effect of the Time Constant τ on Exponential Decay")
plt.xlabel("Time (t)")
plt.ylabel("Amplitude e^(-t/τ)")
plt.grid(True)
plt.legend()
plt.axhline(1/np.e, color='r', linestyle='--', label='1/e ≈ 0.37')
plt.legend()
plt.show()
```

::: notes
This plot shows three decaying exponentials with different time constants. The blue curve has the smallest tau, and you can see it decays most quickly. The green curve has the largest tau and decays very slowly. The red dashed line shows the 1/e level, so you can see where each curve crosses that line at `t = tau`.
:::

## Slide 30: The Complex Exponential: `e^{j\omega_0 t}`
- Now, let `a` be purely imaginary: `a = j\omega_0`.
  $$x(t) = e^{j\omega_0 t}$$
- Using **Euler's Relation**, we can decompose this into real sinusoidal components:
  $$e^{j\omega_0 t} = \cos(\omega_0 t) + j\sin(\omega_0 t)$$
- This signal is **periodic** with fundamental period `T_0 = 2\pi / |\omega_0|`.

::: notes
The magic happens when we let the exponent 'a' be purely imaginary. The signal `e^(j*omega*t)` is called a complex exponential. Euler's relation tells us that this signal is actually composed of a real part, which is a cosine, and an imaginary part, which is a sine. This signal is fundamentally periodic.
:::

## Slide 31: Why is `e^{j\omega_0 t}` Important?
- It is the mathematical basis for Fourier analysis.
- It represents signals that oscillate at a single frequency, `\omega_0`.
- The real and imaginary parts are sinusoids, which are fundamental to describing oscillations in nature (e.g., pendulums, AC circuits, sound waves).

::: notes
Why do we care so much about this signal? Because it is the atom of oscillation. It represents a pure, single frequency. The theory of Fourier analysis, which we will study in depth, is built on the idea that we can represent *any* periodic signal as a sum of these complex exponentials. They are the fundamental building blocks of periodic phenomena.
:::

## Slide 32: Visualizing `e^{j\omega_0 t}`: The Corkscrew
To truly understand the complex exponential, we can visualize it in 3D. The axes are Time, the Real part, and the Imaginary part. The signal traces out a corkscrew or helix shape as time progresses.

::: notes
It can be hard to visualize a complex-valued signal. One of the best ways is to plot it in three dimensions. We let one axis be time, one be the real part (`cos(t)`), and the third be the imaginary part (`sin(t)`). When you do this, the signal traces out a beautiful helix, like a corkscrew, advancing in time.
:::

## Slide 33: 3D Corkscrew Plot
```{r 3d_plot, echo=FALSE}
t <- seq(0, 4*pi, length.out=200)
df_3d <- data.frame(
  Time = t,
  Real = cos(t),
  Imag = sin(t)
)

plot_ly(df_3d, x = ~Time, y = ~Real, z = ~Imag, type = 'scatter3d', mode = 'lines',
        line = list(width = 4, color = ~Time, colorscale = 'Viridis')) %>%
  layout(title = "The Corkscrew of e^(jt)")
```

::: notes
Here is the 3D plot. You can see the helical path. The projection of this helix onto the Time-Real plane is a cosine wave. The projection onto the Time-Imaginary plane is a sine wave. This visual confirms the relationship in Euler's formula.
:::

## Slide 34: Discrete-Time Periodicity
- The discrete-time complex exponential is `x[n] = e^{j\omega_0 n}`.
- Unlike the CT case, this signal is **not** periodic for all values of `\omega_0`.
- For `x[n]` to be periodic with period `N`, we require:
  $$e^{j\omega_0 (n+N)} = e^{j\omega_0 n}$$
- This implies that `\omega_0 N` must be an integer multiple of `2\pi`.
  $$\frac{\omega_0}{2\pi} = \frac{m}{N}$$
- Periodicity only holds if the frequency `\omega_0 / (2\pi)` is a **rational number**.

::: notes
Now let's consider the discrete-time version, `e^(j*omega*n)`. And here we find a major difference from the continuous-time case. A discrete-time exponential is only periodic if its frequency is a rational number. This is a subtle but critical point.
:::

## Slide 35: The "Aliasing" Trap in Discrete Time
- Another key difference: In discrete time, frequencies separated by `2\pi` are identical!
  $$e^{j(\omega_0 + 2\pi)n} = e^{j\omega_0 n} e^{j2\pi n} = e^{j\omega_0 n} \times 1$$
- This means that a high frequency can look exactly like a low frequency after sampling. This phenomenon is called **aliasing**.
- All unique frequencies for DT signals exist in an interval of `2\pi`, e.g., `[0, 2\pi)` or `[-\pi, \pi)`.

::: notes
Here's another strange and important property of discrete-time signals. Frequencies that are `2*pi` apart are indistinguishable. Adding `2*pi` to the frequency of a discrete exponential gives you back the exact same sequence. This leads to aliasing, where a high-frequency continuous signal, once sampled, can impersonate a lower-frequency signal. This is a major concern in digital signal processing.
:::

## Slide 36: Visualizing Aliasing
Let's plot two different frequencies: `\omega_1 = \pi/4` and `\omega_2 = 9\pi/4`.
Note that `\omega_2 = \omega_1 + 2\pi`.

```{r aliasing_plot, echo=FALSE, fig.width=8, fig.height=4}
n <- 0:15
w1 <- pi/4
w2 <- 9*pi/4
df_alias <- data.frame(
  n = n,
  y1 = cos(w1 * n),
  y2 = cos(w2 * n)
)

# Continuous underlying waves
t <- seq(0, 15, length.out=500)
df_cont_alias <- data.frame(
  t = t,
  y1_cont = cos(w1 * t),
  y2_cont = cos(w2 * t)
)

ggplot(df_cont_alias, aes(x=t)) +
  geom_line(aes(y=y1_cont), color="lightblue") +
  geom_line(aes(y=y2_cont), color="lightcoral", linetype="dashed") +
  geom_point(data=df_alias, aes(x=n, y=y1), color="blue", size=3, shape=16) +
  geom_point(data=df_alias, aes(x=n, y=y2), color="red", size=3, shape=1) +
  labs(title="Aliasing: cos(πn/4) and cos(9πn/4)",
       subtitle="Blue (solid) is ω=π/4. Red (dashed) is ω=9π/4. The sample points are identical.",
       x="n", y="Value") +
  theme_minimal()

```
::: notes
This plot is the proof. The solid blue line is the low-frequency cosine. The dashed red line is the high-frequency cosine. But look at the sample points—the blue circles and the red hollow circles. They land on the exact same values. For the discrete-time system, these two vastly different continuous signals are completely indistinguishable.
:::

## Slide 37: Summary of Part 3
- The exponential `Ce^{at}` is a fundamental signal family.
- **Real exponentials** (`a` is real) model growth and decay, governed by the **time constant τ**.
- **Complex exponentials** (`a` is imaginary) model pure oscillations (sinusoids).
  - Visualized as a 3D corkscrew.
- **Discrete-time exponentials** have unique properties:
  - Only periodic if frequency is a rational number.
  - Frequencies separated by `2\pi` are identical (**aliasing**).

::: notes
In this section, we've introduced the exponential signal family. We've seen how real exponents lead to growth and decay, and how pure imaginary exponents lead to perpetual oscillation. We also highlighted the crucial and sometimes counter-intuitive properties of their discrete-time counterparts, including the conditions for periodicity and the concept of aliasing.
:::

# Part 4: Singularity Functions

## Slide 38: The Unit Impulse and Unit Step
- Singularity functions are an important class of idealized signals.
- They are used as basic building blocks to construct more complex signals.
- The two key singularity functions are:
  1. The **Unit Impulse** (`\delta`)
  2. The **Unit Step** (`u`)

::: notes
We now move on to a set of idealized signals called singularity functions. They are not something you can perfectly create in a lab, but they are incredibly powerful mathematical tools. They serve as the fundamental building blocks for representing other signals and analyzing systems. The two most important are the unit impulse and the unit step.
:::

## Slide 39: The Discrete-Time Unit Impulse and Step
- **Unit Impulse `\delta[n]`** (also called the unit sample):
  $$\delta[n] = \begin{cases} 1 & n=0 \\ 0 & n \neq 0 \end{cases}$$
- **Unit Step `u[n]`**:
  $$u[n] = \begin{cases} 1 & n \ge 0 \\ 0 & n < 0 \end{cases}$$

::: notes
In discrete time, these functions are quite simple. The unit impulse, delta[n], is a signal that is 1 at n=0 and zero everywhere else. It's a single, instantaneous blip. The unit step, u[n], is a signal that is zero for all negative time, and then switches on to 1 at n=0 and stays there forever.
:::

## Slide 40: Relationship between `\delta[n]` and `u[n]`
- The unit impulse is the **first difference** of the unit step:
  $$\delta[n] = u[n] - u[n-1]$$
- The unit step is the **running sum** of the unit impulse:
  $$u[n] = \sum_{k=-\infty}^{n} \delta[k]$$

This relationship is crucial for system analysis.

::: notes
There is a fundamental relationship between the impulse and the step. The impulse is the difference between a step and a slightly delayed step—it captures the moment of change. Conversely, the step is the running sum of an impulse. You start with nothing, add the impulse at n=0, and you get the step function.
:::

## Slide 41: Visualizing the "Running Sum" (1/3): `\delta[n]`
The starting point: a single impulse at `n=0`.

```{r running_sum1, echo=FALSE, fig.width=8, fig.height=4}
df_delta <- data.frame(n=-5:5, y=c(0,0,0,0,0,1,0,0,0,0,0))
ggplot(df_delta, aes(x=n, y=y)) +
  geom_segment(aes(xend=n, yend=0)) +
  geom_point() +
  labs(title="Signal: δ[n]", x="n", y="Value") + theme_minimal() + ylim(0,1.2)
```

::: notes
Let's build the step function from impulses. We start with a single impulse at the origin, `delta[n]`.
:::

## Slide 42: Visualizing the "Running Sum" (2/3): `\delta[n] + \delta[n-1]`
Adding a delayed impulse. The running sum is accumulating.

```{r running_sum2, echo=FALSE, fig.width=8, fig.height=4}
df_deltas <- data.frame(n=-5:5, y=c(0,0,0,0,0,1,1,0,0,0,0))
ggplot(df_deltas, aes(x=n, y=y)) +
  geom_segment(aes(xend=n, yend=0)) +
  geom_point() +
  labs(title="Running Sum up to n=1: Σ δ[k]", x="n", y="Value") + theme_minimal() + ylim(0,1.2)
```

::: notes
Now, we add the next impulse in the sequence, `delta[n-1]`. The running sum now has a value of 1 at n=0 and n=1. We are building the step.
:::

## Slide 43: Visualizing the "Running Sum" (3/3): `u[n]`
The full running sum, `u[n] = \sum_{k=0}^{n} \delta[n-k]`, results in the unit step.

```{r running_sum3, echo=FALSE, fig.width=8, fig.height=4}
df_u <- data.frame(n=-5:5, y=c(0,0,0,0,0,1,1,1,1,1,1))
ggplot(df_u, aes(x=n, y=y)) +
  geom_segment(aes(xend=n, yend=0)) +
  geom_point() +
  labs(title="The Full Running Sum: u[n]", x="n", y="Value") + theme_minimal() + ylim(0,1.2)
```

::: notes
As we continue this process for all positive time, the running sum fills in, and we get the unit step function. This shows that the step is composed of a superposition of delayed impulses.
:::

## Slide 44: The Continuous-Time Unit Impulse `\delta(t)`
- The continuous-time impulse `\delta(t)` is more abstract.
- It is defined by its properties, not its value. It is NOT infinity at t=0.
- It is viewed as the limit of a rectangular pulse with **area = 1** as the width approaches zero.
  - Width `\Delta`, Height `1/\Delta`.
- Key Property: **The Sifting Property**
  $$\int_{-\infty}^{\infty} x(t)\delta(t-t_0)dt = x(t_0)$$

::: notes
The continuous-time impulse is a more challenging concept. You can't just define its value at t=0. Instead, we define it as a limiting process. Imagine a rectangle of width Delta and height 1/Delta. Its area is always 1. The unit impulse is what you get as you squeeze the width to zero. The most important property is "sifting"—it samples the value of another function at a specific point.
:::

## Slide 45: The "Rectangular Approximation" (1/3): `\Delta = 0.5`
A pulse of width 0.5 and height 2. Area = 1.

```{python rect_approx1, echo=FALSE}
import numpy as np
import matplotlib.pyplot as plt

def rect_pulse(t, delta):
    return (1/delta) * ((t >= -delta/2) & (t <= delta/2))

t = np.linspace(-1, 1, 500)
plt.figure(figsize=(8,4))
plt.plot(t, rect_pulse(t, 0.5))
plt.title("Rectangular Approximation of δ(t): Δ = 0.5")
plt.xlabel("Time (t)")
plt.ylabel("Amplitude")
plt.ylim(0, 110)
plt.grid(True)
plt.show()

```

::: notes
Let's visualize this limiting process. Here's our pulse with a width of 0.5. To maintain unit area, its height must be 2.
:::

## Slide 46: The "Rectangular Approximation" (2/3): `\Delta = 0.1`
The pulse gets narrower and taller, but the area remains 1.

```{python rect_approx2, echo=FALSE}
t = np.linspace(-1, 1, 500)
plt.figure(figsize=(8,4))
plt.plot(t, rect_pulse(t, 0.1))
plt.title("Rectangular Approximation of δ(t): Δ = 0.1")
plt.xlabel("Time (t)")
plt.ylabel("Amplitude")
plt.ylim(0, 110)
plt.grid(True)
plt.show()
```

::: notes
Now we shrink the width to 0.1. The height must increase to 10 to keep the area equal to 1.
:::

## Slide 47: The "Rectangular Approximation" (3/3): `\Delta = 0.01`
As `\Delta \to 0`, the pulse approaches an impulse.

```{python rect_approx3, echo=FALSE}
t = np.linspace(-1, 1, 500)
plt.figure(figsize=(8,4))
plt.plot(t, rect_pulse(t, 0.01))
plt.title("Rectangular Approximation of δ(t): Δ = 0.01")
plt.xlabel("Time (t)")
plt.ylabel("Amplitude")
plt.ylim(0, 110)
plt.grid(True)
plt.show()
```

::: notes
Finally, with a width of 0.01, the height shoots up to 100. In the limit, as the width goes to zero, we get the idealized unit impulse: a function with zero width, infinite height, but a precisely defined area of 1.
:::


## Slide 48: Summary of Part 4
- **Singularity functions** (`\delta` and `u`) are idealized signals used as building blocks.
- **Discrete Time:**
  - `\delta[n]` is a single non-zero point.
  - `u[n]` is the running sum of `\delta[n]`.
- **Continuous Time:**
  - `\delta(t)` is defined as the limit of a unit-area pulse.
  - Its most important characteristic is the **sifting property**.

::: notes
To recap, singularity functions like the impulse and the step are fundamental tools. In discrete time, they are simple to define and visualize. In continuous time, the impulse is more abstract, defined by its sifting property and as the limit of a pulse of unit area.
:::

# Part 5: System Properties

## Slide 49: What is a System?
- A system is a process that transforms input signals into output signals.
- We represent it as:
  - `y(t) = T\{x(t)\}`
  - `y[n] = T\{x[n]\}`
- Examples:
  - An audio amplifier (input: weak audio signal, output: strong audio signal).
  - A cruise control system (input: desired speed, output: engine throttle).
  - An image filter (input: original image, output: enhanced image).

::: notes
Finally, we turn our attention from signals to systems. A system is anything that takes an input signal and produces an output signal. It's a transformation. We'll spend the rest of this course studying different kinds of systems, but first, we need a language to describe their properties.
:::

## Slide 50: System Properties
We can classify systems based on several key properties:
1. Memory
2. Invertibility
3. Causality
4. Stability
5. **Linearity**
6. **Time-Invariance**

The last two, Linearity and Time-Invariance, are the most important for this course.

::: notes
We classify systems based on their fundamental properties. Does it have memory? Can its effects be undone? Does it react only to past events? Does it produce bounded outputs for bounded inputs? And most critically for our purposes: is it linear, and is it time-invariant?
:::

## Slide 51: Linearity
A system is **linear** if it possesses the property of **superposition**.
1. **Additivity:** `T\{x_1(t) + x_2(t)\} = T\{x_1(t)\} + T\{x_2(t)\}`
2. **Homogeneity (Scaling):** `T\{ax(t)\} = aT\{x(t)\}`

In words: The response to a weighted sum of inputs is the weighted sum of the individual responses.

::: notes
Linearity is a crucial property. It means the system obeys superposition. If you put the sum of two signals in, you get the sum of their individual outputs out. If you scale the input by a factor, the output is scaled by the same factor. This principle dramatically simplifies the analysis of complex systems.
:::

## Slide 52: Visual Proof of a Nonlinear System
Let's test the system `y = T\{x\} = x^2` for linearity. We will check if `T\{x_1+x_2\} = T\{x_1\}+T\{x_2\}`.

```{python linearity_test, echo=FALSE}
import numpy as np
import matplotlib.pyplot as plt

t = np.linspace(0, 2*np.pi, 100)
x1 = np.sin(t)
x2 = 0.5 * np.cos(2*t)

# T(x1 + x2)
y_sum_first = (x1 + x2)**2

# T(x1) + T(x2)
y_sum_after = x1**2 + x2**2

plt.figure(figsize=(10, 5))
plt.plot(t, y_sum_first, label='T(x1 + x2) = (x1+x2)^2', lw=2)
plt.plot(t, y_sum_after, label='T(x1) + T(x2) = x1^2 + x2^2', linestyle='--')
plt.title("Testing Linearity for y = x^2")
plt.xlabel("Time (t)")
plt.ylabel("Output")
plt.grid(True)
plt.legend()
plt.show()
```

::: notes
A visual test can be very convincing. Here we test the system `y = x^2`. We take two inputs, x1 and x2. The solid line is the output when we first add the inputs and then apply the system. The dashed line is the output when we apply the system to each input first, and then add the outputs. Because the two lines do not match, the superposition principle fails. This system is not linear.
:::

## Slide 53: Time-Invariance
- A system is **time-invariant** if its behavior does not change over time.
- If `y(t) = T\{x(t)\}`, then a time-invariant system must satisfy:
  `y(t - t_0) = T\{x(t - t_0)\}`
- In words: If you shift the input in time, the output is the exact same shape, just shifted by the same amount. The system doesn't care *when* you apply the input.

::: notes
The second critical property is time-invariance. This means the system's rules are fixed. The behavior you see today is the same behavior you would see tomorrow. If you feed a signal into your stereo today, and then feed the exact same signal in tomorrow but delayed by 24 hours, you expect to hear the exact same output, also delayed by 24 hours. That is time-invariance.
:::

## Slide 54: Visualizing Time-Invariance
- Input a pulse at `t=0`, record the output.
- Input the same pulse, but shifted to `t=5`.
- If the system is time-invariant, the second output must be identical to the first, just shifted to start at `t=5`.

```{r ti_viz, echo=FALSE, fig.width=10, fig.height=4}
# Simulate a simple "smearing" system (e.g., a simple moving average)
smear <- function(x) {
  stats::filter(x, rep(1/5, 5), sides=2)
}

t <- seq(-2, 10, length.out=500)
# Input pulse at t=0
x1 <- ifelse(t >= 0 & t <= 1, 1, 0)
y1 <- smear(x1)
# Input pulse at t=5
x2 <- ifelse(t >= 5 & t <= 6, 1, 0)
y2 <- smear(x2)

df_ti <- data.frame(t, y1, y2)

p1 <- ggplot(df_ti, aes(x=t, y=y1)) + geom_line(color="blue") +
  labs(title="Output for Input at t=0", x="t", y="y1(t)") + theme_minimal() + xlim(-2, 10)
p2 <- ggplot(df_ti, aes(x=t, y=y2)) + geom_line(color="red") +
  labs(title="Output for Input at t=5", x="t", y="y2(t)") + theme_minimal() + xlim(-2, 10)

p1 + p2
```

::: notes
Let's see what this looks like. We have a hypothetical system that "smears" an input pulse. On the left, we input a pulse at t=0, and we get the smeared blue output. On the right, we input the same pulse but delayed to t=5. The system produces the red output. As you can see, the red output is just a shifted version of the blue one. The shape is identical. This system is time-invariant.
:::

## Slide 55: LTI Systems
- Systems that are both **Linear** and **Time-Invariant** are called **LTI systems**.
- This class of systems is immensely important because:
  1. They can be analyzed in great detail with a powerful set of mathematical tools (e.g., convolution, Fourier and Laplace transforms).
  2. Many real-world physical systems can be accurately modeled as LTI systems.
- The rest of this course will focus heavily on LTI systems.

::: notes
When a system is both linear AND time-invariant, we call it an LTI system. This is the sweet spot. LTI systems are the focus of our study because they are simple enough to be described by powerful mathematics, yet complex enough to model a huge range of important physical processes, from circuits to mechanical vibrations to communication channels.
:::

## Slide 56: Course Summary
- We started with the fundamentals of **signals** (CT/DT, energy/power).
- We explored signal **transformations** (shifting, scaling, even/odd).
- We dove into the crucial **exponential signal family**.
- We defined idealized **singularity functions** (`\delta` and `u`).
- We finished by classifying **systems** based on their properties, identifying the importance of **LTI Systems**.

Thank you.

::: notes
To conclude our first chapter, we have built a solid foundation. We know what signals are and how to classify them. We can manipulate them. We have met the all-important exponential signals and the useful impulse and step functions. And finally, we have a framework for describing and classifying systems, which has led us to the most important class of all: LTI systems. Our journey from here will be to build the analytical tools to master them.
:::
